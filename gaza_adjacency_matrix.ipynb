{
 "metadata": {
  "name": "",
  "signature": "sha256:297a935bf45c65fb77f09636f112a5be513dcfc5a62009a5f9a1a498842429e0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Data Prep for Gaza Top Media Source Word Adjacency Matrix\n",
      "<hr>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from networkx.readwrite import json_graph\n",
      "import networkx as nx\n",
      "import requests\n",
      "import mediacloud\n",
      "import json\n",
      "import os\n",
      "import copy\n",
      "import cPickle\n",
      "\n",
      "# Berkman Projects Directory\n",
      "berkman_projects = os.environ['BKP']\n",
      "api_key = cPickle.load( file( os.path.expanduser( berkman_projects + '/MediaCloud/mediacloud_api_key.pickle' ), 'r' ) )\n",
      "mc = mediacloud.api.MediaCloud(api_key)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Top Gaza Media Sources by In-Link (Twitter (#2) removed)\n",
      "# Represents all Media Sources w/ greater than 200 In-Links\n",
      "\n",
      "# Media_id Name\n",
      "# 1        New York Times\n",
      "# 1751     Guardian\n",
      "# 23209    jpost.com\n",
      "# 38702    maannews-en\n",
      "# 4442     Reuters News\n",
      "# 1094     BBC\n",
      "# 20129    haaretz.com\n",
      "# 34082    timesofisrael.com\n",
      "# 2        Washington Post\n",
      "# 39008    reliefWeb\n",
      "\n",
      "top_10_media = [1, 1751, 23209, 38702, 4442, 1094, 20129, 34082, 2, 39008]\n",
      "# top_stories = {}\n",
      "top_words = {}\n",
      "for m in top_10_media:\n",
      "    top_words[m] = mc.wordCount('{~ controversy:720 } and media_id:%s' % (m), num_words=25)\n",
      "#     top_stories[m] = mc.storyList('{~ controversy:720 } and media_id:%s' % (m), '', 0, 500)\n",
      "\n",
      "# all_stories = []\n",
      "# for m in top_10_media:\n",
      "#     for s in top_stories[m]:\n",
      "#         all_stories.append(s)\n",
      "\n",
      "print('Aggregated %d stories from %d media sources.') % (len(all_stories), len(top_10_media))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Aggregated 3292 stories from 10 media sources.\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1.) Build unique word list across media sources based on stem (aggregate counts)\n",
      "# 2.) Strip out most common words to avoid obvious correlations\n",
      "# 3.) Merge with D3 data format\n",
      "\n",
      "stop_words = ['content_category', 'gaza', 'haaretz', 'in_url_headline', 'israel', 'israeli', 'true', 'war', 'bbc', 'palestinian', 'minister', 'yagna', 'tel', 'aviv', 'reuters', 'abu', 'isis', 'blog_name', 'violations', 'iran', 'iraq', 'explode', 'yanir', 'sirens', 'defence', 'guardian', 'intercepts', 'mohammed', 'customfields', 'bethlehem', 'coordination', 'exploded', 'iof', 'post_id', 'warplanes', 'binyamin']\n",
      "\n",
      "top_word_list = {}\n",
      "for m in top_10_media:\n",
      "    for w in top_words[m]:\n",
      "        if w['stem'] in top_word_list:\n",
      "            top_word_list[w['stem']]['count'] += w['count']\n",
      "        else:\n",
      "            top_word_list[w['stem']] = copy.deepcopy(w)\n",
      "            \n",
      "wordSet = [top_word_list[key] for key in filter(lambda x: top_word_list[x]['term'] not in stop_words, top_word_list)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data template for D3 node/link format\n",
      "data = {\n",
      "    \"nodes\":[{\"name\":word['term'],\"group\":1} for word in filter(lambda x: x['term'] not in stop_words, wordSet)],\n",
      "    \"links\":[]\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build unique word count list from story texts and populate D3 data format w/ terms and ranks\n",
      "word_network = nx.DiGraph()\n",
      "\n",
      "links = {}\n",
      "for s in all_stories:\n",
      "    story_text = s['story_text'].lower()\n",
      "    cur_words = [w['term'] for w in filter(lambda x: story_text.count(x['term']) > 0 and x['term'] not in stop_words, wordSet)]\n",
      "    for a in cur_words:\n",
      "        for b in cur_words:\n",
      "            if a+','+b in links:\n",
      "                links[a+','+b] += 1\n",
      "            else:\n",
      "                links[a+','+b] = 1\n",
      "\n",
      "just_terms = [w['term'] for w in wordSet]\n",
      "weights = {w['term']: w['count'] for w in wordSet}\n",
      "ranks = {word['term']:rank for rank, word in enumerate(wordSet)}\n",
      "\n",
      "for a in just_terms:\n",
      "    for b in just_terms:\n",
      "        if a+','+b in links:\n",
      "            if(links[a+','+a] != 0 or links[b+','+b] != 0):\n",
      "                word_network.add_node(a, weight = weights[a])\n",
      "                word_network.add_node(b, weight = weights[b])\n",
      "                word_network.add_edge(a, b)\n",
      "                data[\"links\"].append({\"source\":ranks[a],\"target\":ranks[b],\"value\":(links[a+','+b]+links[b+','+a])}) # /(links[a+','+a]/2+links[b+','+b]/2)})\n",
      "#             data[\"links\"].append({\"source\":ranks[a],\"target\":ranks[b],\"value\":(links[a+','+b]+links[b+','+a])/(links[a+','+a]/2+links[b+','+b]/2)})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dump to disk\n",
      "with open('gaza_matrix.json', 'wb') as fp:\n",
      "    json.dump(data, fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Determine domain of values to map to opacity\n",
      "weights = [x['value'] for x in data['links']]\n",
      "print('Link Weight Range: %d-%d') % (min(weights), max(weights))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Link Weight Range: 16-3604\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metadata = {'metadata': {'min-weight': min(weights), 'max-weight': max(weights), 'word-count': len(just_terms)}}\n",
      "json.dumps(metadata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 103,
       "text": [
        "'{\"metadata\": {\"word-count\": 55, \"max-weight\": 3604, \"min-weight\": 16}}'"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_gexf(word_network, 'gaza_word_network_01.gexf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    }
   ],
   "metadata": {}
  }
 ]
}