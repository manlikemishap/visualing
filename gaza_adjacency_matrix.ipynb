{
 "metadata": {
  "name": "",
  "signature": "sha256:f2f645475e128db6ff85949833e23ea88545292650b7934c89c405966d50049b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Data Prep for Gaza Top Media Source Word Adjacency Matrix\n",
      "<hr>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from networkx.readwrite import json_graph\n",
      "import networkx as nx\n",
      "import requests\n",
      "import mediacloud\n",
      "import json\n",
      "import os\n",
      "import copy\n",
      "import cPickle\n",
      "\n",
      "# Berkman Projects Directory\n",
      "berkman_projects = os.environ['BKP']\n",
      "api_key = cPickle.load( file( os.path.expanduser( berkman_projects + '/MediaCloud/mediacloud_api_key.pickle' ), 'r' ) )\n",
      "mc = mediacloud.api.MediaCloud(api_key)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Top Gaza Media Sources by In-Link (Twitter (#2) removed)\n",
      "# Represents all Media Sources w/ greater than 200 In-Links\n",
      "\n",
      "# Media_id Name\n",
      "# 1        New York Times\n",
      "# 1751     Guardian\n",
      "# 23209    jpost.com\n",
      "# 38702    Ma'an News (English)\n",
      "# 4442     Reuters News\n",
      "# 1094     BBC\n",
      "# 20129    haaretz.com\n",
      "# 34082    timesofisrael.com\n",
      "# 2        Washington Post\n",
      "# 39008    ReliefWeb\n",
      "# 1148     Al Jazeera (English)\n",
      "# 22728    Electronic Intifada\n",
      "# 64600    MondoWeiss               79981 - mondoweiss.observer.com (spidered:spidered) - going to be merged?\n",
      "# 40915    YNet News                24741 - YNetNews.com (spidered:spidered)\n",
      "\n",
      "media_sources = {\n",
      "    1: \"New York Times\",\n",
      "    1751: \"Guardian\",\n",
      "    23209: \"The Jerusalem Post\",\n",
      "    38702: \"Ma'an News\",\n",
      "    4442: \"Reuters News\",\n",
      "    1094: \"BBC\",\n",
      "    20129: \"Haaretz\",\n",
      "    34082: \"The Times of Israel\",\n",
      "    2: \"The Washington Post\",\n",
      "    39008: \"ReliefWeb\",\n",
      "    1148: \"Al Jazeera\",\n",
      "    22728: \"Electronic Intifada\",\n",
      "    64600: \"MondoWeiss\",\n",
      "    40915: \"YNet News\"\n",
      "}\n",
      "\n",
      "# For this and future iterative viz data scripts, create array of generation objects with all of the variables \n",
      "# encapsulated in one generation object and reference them all in the code so that there is a record of \n",
      "# all the settings for that iteration.\n",
      "\n",
      "generation_md = {\n",
      "    'gen': '4',\n",
      "    'top_word_count': 100\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Refactor \"top_10_media\". No longer 10.\n",
      "\n",
      "# top_10_media = [1, 1751, 23209, 38702, 4442, 1094, 20129, 34082, 2, 39008]\n",
      "top_10_media = media_sources.keys()\n",
      "\n",
      "# top_stories = {}\n",
      "top_words = {}\n",
      "for m in top_10_media:\n",
      "    top_words[m] = mc.wordCount('{~ controversy:720 } and media_id:%s' % (m), num_words=generation_md['top_word_count'])\n",
      "    print('Got %d words from %s' % (len(top_words[m]), media_sources[m]))\n",
      "#     top_stories[m] = mc.storyList('{~ controversy:720 } and media_id:%s' % (m), '', 0, 500)\n",
      "\n",
      "\n",
      "\n",
      "# all_stories = []\n",
      "# for m in top_10_media:\n",
      "#     for s in top_stories[m]:\n",
      "#         all_stories.append(s)\n",
      "\n",
      "print('Aggregated %d stories from %d media sources.') % (len(all_stories), len(top_10_media))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got 100 words from New York Times\n",
        "Got 100 words from The Washington Post"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from BBC"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from Electronic Intifada"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from YNet News"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from Guardian"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from MondoWeiss"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from Reuters News"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from ReliefWeb"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from Haaretz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from The Times of Israel"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from The Jerusalem Post"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 100 words from Ma'an News"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Got 0 words from Al Jazeera"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Aggregated 3292 stories from 14 media sources.\n"
       ]
      }
     ],
     "prompt_number": 182
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_words[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 171,
       "text": [
        "[{u'count': 197, u'stem': u'israel', u'term': u'israel'},\n",
        " {u'count': 183, u'stem': u'gaza', u'term': u'gaza'},\n",
        " {u'count': 172, u'stem': u'palestinian', u'term': u'palestinian'},\n",
        " {u'count': 154, u'stem': u'isra', u'term': u'israeli'},\n",
        " {u'count': 149, u'stem': u'hama', u'term': u'hamas'},\n",
        " {u'count': 53, u'stem': u'rocket', u'term': u'rocket'},\n",
        " {u'count': 42, u'stem': u'minist', u'term': u'minister'},\n",
        " {u'count': 32, u'stem': u'milit', u'term': u'militant'},\n",
        " {u'count': 30, u'stem': u'netanyahu', u'term': u'netanyahu'},\n",
        " {u'count': 28, u'stem': u'ceas', u'term': u'cease'},\n",
        " {u'count': 28, u'stem': u'famili', u'term': u'family'},\n",
        " {u'count': 27, u'stem': u'jerusalem', u'term': u'jerusalem'},\n",
        " {u'count': 27, u'stem': u'egypt', u'term': u'egypt'},\n",
        " {u'count': 25, u'stem': u'abba', u'term': u'abbas'},\n",
        " {u'count': 25, u'stem': u'arab', u'term': u'arab'},\n",
        " {u'count': 24, u'stem': u'war', u'term': u'war'},\n",
        " {u'count': 23, u'stem': u'soldier', u'term': u'soldiers'},\n",
        " {u'count': 22, u'stem': u'egyptian', u'term': u'egyptian'},\n",
        " {u'count': 20, u'stem': u'abu', u'term': u'abu'},\n",
        " {u'count': 19, u'stem': u'american', u'term': u'american'},\n",
        " {u'count': 19, u'stem': u'kerri', u'term': u'kerry'},\n",
        " {u'count': 17, u'stem': u'weapon', u'term': u'weapons'},\n",
        " {u'count': 16, u'stem': u'obama', u'term': u'obama'},\n",
        " {u'count': 16, u'stem': u'tunnel', u'term': u'tunnel'},\n",
        " {u'count': 16, u'stem': u'spokesman', u'term': u'spokesman'}]"
       ]
      }
     ],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data template for D3 node/link format\n",
      "data = {\n",
      "    \"nodes\":[{\"name\":word['term'],\"group\":1} for word in filter(lambda x: x['term'] not in stop_words, wordSet)],\n",
      "    \"links\":[]\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build unique word count list from story texts and populate D3 data format w/ terms and ranks\n",
      "word_network = nx.DiGraph()\n",
      "\n",
      "links = {}\n",
      "for s in all_stories:\n",
      "    story_text = s['story_text'].lower()\n",
      "    cur_words = [w['term'] for w in filter(lambda x: story_text.count(x['term']) > 0 and x['term'] not in stop_words, wordSet)]\n",
      "    for a in cur_words:\n",
      "        for b in cur_words:\n",
      "            if a+','+b in links:\n",
      "                links[a+','+b] += 1\n",
      "            else:\n",
      "                links[a+','+b] = 1\n",
      "\n",
      "just_terms = [w['term'] for w in wordSet]\n",
      "weights = {w['term']: w['count'] for w in wordSet}\n",
      "ranks = {word['term']:rank for rank, word in enumerate(wordSet)}\n",
      "\n",
      "for a in just_terms:\n",
      "    for b in just_terms:\n",
      "        if a+','+b in links:\n",
      "            if(links[a+','+a] != 0 or links[b+','+b] != 0):\n",
      "                word_network.add_node(a, weight = weights[a])\n",
      "                word_network.add_node(b, weight = weights[b])\n",
      "                word_network.add_edge(a, b)\n",
      "                data[\"links\"].append({\"source\":ranks[a],\"target\":ranks[b],\"value\":(links[a+','+b]+links[b+','+a])}) # /(links[a+','+a]/2+links[b+','+b]/2)})\n",
      "#             data[\"links\"].append({\"source\":ranks[a],\"target\":ranks[b],\"value\":(links[a+','+b]+links[b+','+a])/(links[a+','+a]/2+links[b+','+b]/2)})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Build Media-Source / Word Co-Occurrence Network"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Media Source / Word Network\n",
      "msw_network = nx.DiGraph()\n",
      "\n",
      "for m in top_10_media:\n",
      "    for w in top_words[m]:\n",
      "        msw_network.add_node(w['term'], type='word')\n",
      "        msw_network.add_node(media_sources[m], type='media_source')\n",
      "        msw_network.add_edge(media_sources[m], w['term'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 186
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Writing %d nodes to network.' % (msw_network.number_of_nodes()))\n",
      "nx.write_gexf(msw_network, 'gaza_msw_network_%s.gexf' % (generation_md['gen']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing 471 nodes to network.\n"
       ]
      }
     ],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print(top_words[1][0]['term'])\n",
      "\n",
      "print(top_words[1][0])\n",
      "print(just_terms[0])\n",
      "print(type(weights))\n",
      "\n",
      "print(weights[just_terms[0]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "israel\n",
        "{u'count': 197, u'term': u'israel', u'stem': u'israel'}\n",
        "violence\n",
        "<type 'dict'>\n",
        "36\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dump to disk\n",
      "with open('gaza_matrix.json', 'wb') as fp:\n",
      "    json.dump(data, fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Determine domain of values to map to opacity\n",
      "weights = [x['value'] for x in data['links']]\n",
      "print('Link Weight Range: %d-%d') % (min(weights), max(weights))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Link Weight Range: 16-3604\n"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metadata = {'metadata': {'min-weight': min(weights), 'max-weight': max(weights), 'word-count': len(just_terms)}}\n",
      "json.dumps(metadata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 168,
       "text": [
        "'{\"metadata\": {\"word-count\": 55, \"max-weight\": 3604, \"min-weight\": 16}}'"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_gexf(word_network, 'gaza_word_network_01.gexf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 169
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Output Controversy URLs for URLopedia Social Metrics\n",
      "## To Nathan\n",
      "<hr>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "def fetch_all_stories(solr_query):\n",
      "    start = 0\n",
      "    offset = 1000\n",
      "    all_stories = []\n",
      "    start_time = time.time()\n",
      "    while True:\n",
      "        stories = mc.storyList(solr_query=solr_query, last_processed_stories_id=start, rows=offset)\n",
      "        print offset\n",
      "        all_stories.extend(stories)\n",
      "        if len(stories) < 1:\n",
      "            break\n",
      "        start = max([s['processed_stories_id'] for s in stories])\n",
      "    \n",
      "    return all_stories\n",
      "\n",
      "# stories = fetch_all_stories('{~ controversy:720 }')\n",
      "print('%d Stories Found' % (len(stories)))\n",
      "with open('gaza_urls.csv', 'wb') as url_out:\n",
      "    for s in stories:\n",
      "        url_out.write(s['stories_id'].encode('utf-8') + ', ' + s['url'].encode('utf-8') + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41064 Stories Found\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mc.wordCount('{~ controversy:720 } and media_id:1148')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 185,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 185
    }
   ],
   "metadata": {}
  }
 ]
}